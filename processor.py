import os
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, List, Set

from decompiler import decompile_class_files
from cleaner import clean_java_file
from jar_utils import process_jar_file


class JarProcessor:
    """
    High-performance processor for analyzing and cleaning infected JAR files.
    Implements parallel processing and comprehensive malware detection.
    """

    def __init__(self, logger: Optional[logging.Logger] = None) -> None:
        """
        Initialize processor with optional custom logger.

        Args:
            logger: Optional custom logger instance
        """
        self.logger = logger or logging.getLogger(__name__)
        self.processed_files: Set[str] = set()
        self.failed_files: List[str] = []

    async def _process_single_jar(self, jar_path: str) -> bool:
        """
        Process a single JAR file asynchronously.

        Args:
            jar_path: Path to JAR file

        Returns:
            bool: True if processing successful
        """
        try:
            success = await process_jar_file(jar_path)
            if success:
                self.processed_files.add(jar_path)
                self.logger.info(f"Successfully processed {jar_path}")
            else:
                self.failed_files.append(jar_path)
                self.logger.error(f"Failed to process {jar_path}")
            return success
        except Exception as e:
            self.logger.error(f"Error processing {jar_path}: {e}")
            self.failed_files.append(jar_path)
            return False

    async def _process_jar_batch(self, jar_files: List[str], max_workers: int) -> None:
        """
        Process a batch of JAR files in parallel.

        Args:
            jar_files: List of JAR file paths
            max_workers: Maximum number of parallel workers
        """
        tasks = []
        semaphore = asyncio.Semaphore(max_workers)

        async def bounded_process(jar_path: str) -> None:
            async with semaphore:
                await self._process_single_jar(jar_path)

        for jar_path in jar_files:
            task = asyncio.create_task(bounded_process(jar_path))
            tasks.append(task)

        await asyncio.gather(*tasks)

    def process_directory(self, path: str, parallel: bool = True, max_workers: int = 4) -> None:
        """
        Process all JAR files in a directory, optionally in parallel.

        Args:
            path: Directory path containing JAR files
            parallel: Enable parallel processing
            max_workers: Maximum number of parallel workers
        """
        jar_files = []
        for root, _, files in os.walk(path):
            for file in files:
                if file.endswith('.jar'):
                    jar_files.append(os.path.join(root, file))

        if not jar_files:
            self.logger.info(f"No JAR files found in {path}")
            return

        self.logger.info(f"Found {len(jar_files)} JAR files to process")

        if parallel:
            asyncio.run(self._process_jar_batch(jar_files, max_workers))
        else:
            for jar_path in jar_files:
                asyncio.run(self._process_single_jar(jar_path))

        self.logger.info(f"Processing complete. Processed: {len(self.processed_files)}, "
                         f"Failed: {len(self.failed_files)}")

    @property
    def statistics(self) -> dict:
        """Get processing statistics."""
        return {
            "processed": len(self.processed_files),
            "failed": len(self.failed_files),
            "failed_files": self.failed_files
        }

    def reset_statistics(self) -> None:
        """Reset processing statistics."""
        self.processed_files.clear()
        self.failed_files.clear()