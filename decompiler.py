import asyncio
import logging
import os
import requests
from tqdm import tqdm

logger = logging.getLogger(__name__)

class DecompileProgress:
    """Track progress of decompilation process with tqdm integration."""
    def __init__(self, total):
        self.progress = tqdm(total=total, desc="Decompiling", unit="files")
        self.failed_files = []

    def update(self, file_path, success):
        self.progress.update(1)
        if not success and 'javassist' not in file_path:
            self.failed_files.append(file_path)

    def close(self):
        self.progress.close()

async def decompile_class_file(class_file, output_dir, progress_tracker):
    """Decompile a single class file using CFR decompiler."""
    try:
        process = await asyncio.create_subprocess_exec(
            "java", "-jar", "cfr-0.152.jar",
            class_file,
            "--outputdir", output_dir,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        await process.communicate()
        success = process.returncode == 0
        progress_tracker.update(class_file, success)
        return class_file, success
    except Exception as e:
        progress_tracker.update(class_file, False)
        return class_file, False

def download_cfr():
    """Download the CFR decompiler if not present."""
    cfr_path = "cfr-0.152.jar"
    if not os.path.exists(cfr_path):
        logger.info("CFR decompiler not found. Downloading...")
        cfr_url = "https://www.benf.org/other/cfr/cfr-0.152.jar"
        try:
            response = requests.get(cfr_url, stream=True)
            response.raise_for_status()
            with open(cfr_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            logger.info("Successfully downloaded CFR decompiler")
            return True
        except Exception as e:
            logger.error(f"Failed to download CFR: {e}")
            return False
    return True


async def decompile_class_files(directory):
    """
    Asynchronously decompile all class files in parallel with intelligent batching and error recovery.

    Implementation details:
    - Validates and downloads CFR dependency
    - Implements concurrent batch processing with semaphore control
    - Handles class file discovery with inheritance filtering
    - Provides granular progress tracking with failure isolation

    Args:
        directory (str): Root directory containing class files

    Returns:
        tuple[bool, list]: (Success status, List of failed files)

    Implementation rationale:
    - Uses asyncio.Semaphore to prevent system resource exhaustion
    - Batches class files to optimize I/O operations
    - Isolates decompilation failures to prevent cascade failures
    - Tracks progress with granular status updates
    """
    if not download_cfr():
        return False, []

    # Discover class files with inheritance filtering
    class_files = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".class") and "$" not in file:  # Skip inner classes
                class_files.append(os.path.join(root, file))

    if not class_files:
        logger.info("No class files found to decompile")
        return True, []

    # Initialize progress tracking
    progress_tracker = DecompileProgress(len(class_files))

    # Configure concurrent execution parameters
    MAX_CONCURRENT = min(32, len(class_files))  # Prevent excessive concurrency
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    async def bounded_decompile(class_file):
        """Execute decompilation within concurrency bounds."""
        async with semaphore:
            output_dir = os.path.dirname(class_file)
            return await decompile_class_file(class_file, output_dir, progress_tracker)

    try:
        # Execute parallel decompilation with bounded concurrency
        tasks = [bounded_decompile(cf) for cf in class_files]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results and handle exceptions
        failed_files = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Decompilation failed with error: {result}")
                failed_files.append(str(result))
            elif not result[1]:  # result[1] is success flag
                failed_files.append(result[0])

    except Exception as e:
        logger.error(f"Critical error during decompilation: {e}")
        return False, []
    finally:
        progress_tracker.close()

    if failed_files:
        logger.info(
            f"Failed to decompile {len(failed_files)} non-essential files - "
            f"they will be preserved in the final JAR"
        )

    return True, failed_files